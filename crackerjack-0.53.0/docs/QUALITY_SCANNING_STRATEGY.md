# Crackerjack Quality Scanning: Strategic Decision Framework

## Executive Summary

We need to optimize slow quality hooks (refurb, complexipy, skylos) that take 10+ minutes per scan. Three complementary approaches are available:

1. **Incremental Scanning**: Scan only changed files (10-20x faster)
1. **Mahavishnu Pools**: Parallel tool execution (2.5-3x faster)
1. **Combined Approach**: Incremental + Pools (30-60x faster for typical commits)

**Recommendation**: Implement **Combined Approach** in phases for maximum impact with manageable risk.

______________________________________________________________________

## Current State Analysis

### Tool Performance (Full Repository Scan)

| Tool | Time | Purpose | Alternative |
|------|------|---------|-------------|
| **refurb** | 290s | Python modernization suggestions | None (keep!) |
| **complexipy** | 605s | Cognitive complexity analysis | ruff complexity (10-20x faster) |
| **skylos** | 60s | Dead code detection (Rust) | vulture (Python, 2-5x slower) |
| **semgrep** | 70s | Security vulnerability patterns | None (security critical) |
| **gitleaks** | 29s | Secret detection | None (security critical) |

**Total Slow Tools**: ~1054 seconds (17.5 minutes)

### Tool Classification by Usage Pattern

**Frequently Run (Daily Workflows)**:

- Should be FAST: \<60 seconds
- Tools: ruff, vulture, codespell, check-jsonschema
- Current: ‚úÖ Already fast

**Moderately Frequent (Pre-commit, CI)**:

- Should be MODERATE: 1-3 minutes
- Tools: refurb, skylos, semgrep
- Current: ‚ùå 290s, 60s, 70s (too slow)

**Less Frequent (Publish, Release)**:

- Can be SLOW: 5-10 minutes acceptable
- Tools: complexipy, full security scans
- Current: ‚ö†Ô∏è 605s (too slow even for publish)

______________________________________________________________________

## Decision Matrix

### Option 1: Tool Substitution (Quick Win)

**Approach**: Replace slow tools with faster alternatives for daily workflows.

| Tool | Current | Alternative | Speedup | Trade-off |
|------|---------|-------------|---------|-----------|
| complexipy | 605s | ruff complexity | 10-20x | Fewer complexity metrics |
| skylos | 60s | vulture | 0.5x (slower) | Python vs Rust, different detection |

**Implementation**:

```yaml
# .crackerjack.yaml
tools:
  complexity:
    daily: ruff      # Fast, good enough for daily
    publish: complexipy  # Comprehensive for releases

  dead_code:
    daily: vulture    # Slower but good enough
    publish: skylos   # Fastest, most comprehensive
```

**Pros**:

- ‚úÖ Easy to implement (1 hour)
- ‚úÖ No architectural changes
- ‚úÖ Immediate speedup (10-20x for complexity)

**Cons**:

- ‚ùå Loses some tool capabilities
- ‚ùå Two sets of tool outputs to maintain
- ‚ùå Doesn't solve root problem (full scans)

**Effort**: 1-2 hours
**Impact**: 5-10x speedup for workflows that use complexipy
**Risk**: Low

______________________________________________________________________

### Option 2: Incremental Scanning (High Impact)

**Approach**: Scan only files changed since last successful run.

**Three Implementation Variants**:

#### 2A: Git-Diff-Based (Simplest)

```python
# Scan files changed since HEAD~1
files = git diff --name-only HEAD~1 HEAD
refurb [files]
```

**Effort**: 2-3 hours
**Speedup**: 10-20x for small commits
**Risk**: Low

#### 2B: Marker-Based (Most Accurate)

```python
# Track per-file scan timestamps
if file.hash != last_scanned_hash:
    scan(file)
```

**Effort**: 4-6 hours
**Speedup**: 15-30x (handles edge cases better)
**Risk**: Medium (database management)

#### 2C: Hybrid (Recommended)

```python
# Use git-diff with periodic full scans
if days_since_last_full_scan > 7:
    full_scan()
else:
    incremental_scan()
```

**Effort**: 3-4 hours
**Speedup**: 10-20x (with safety net)
**Risk**: Low

**Pros**:

- ‚úÖ Dramatic speedup for typical commits
- ‚úÖ Works with all tools
- ‚úÖ Minimal behavior change

**Cons**:

- ‚ùå Can miss issues in refactored code
- ‚ùå Requires periodic full scans
- ‚ùå Adds complexity to hook system

______________________________________________________________________

### Option 3: Mahavishnu Pools (Scalability)

**Approach**: Distribute tool execution across worker pools for parallel processing.

**Architecture**:

```
Crackerjack ‚Üí Mahavishnu Pool Manager ‚Üí 8 Parallel Workers
    ‚Üì                                          ‚Üì
  File List                                  refurb (files 1-10)
    ‚Üì                                          ‚Üì
  Task Distributor                           complexipy (files 11-20)
    ‚Üì                                          ‚Üì
  Result Aggregator                         skylos (files 21-30)
                                               ‚Üì
                                            [5 more workers...]
```

**Effort**: 4-6 hours (Phase 1), +4 hours (optimization)
**Speedup**: 2.5-3x for full scans, 3-6x for incremental
**Risk**: Medium (mahavishnu dependency)

**Pros**:

- ‚úÖ Scales to large codebases
- ‚úÖ Works with all tools transparently
- ‚úÖ Fault isolation (worker crashes don't affect others)
- ‚úÖ Can use Kubernetes for massive parallelism

**Cons**:

- ‚ùå Mahavishnu must be running
- ‚ùå More complex architecture
- ‚ùå Debugging distributed execution harder
- ‚ùå Resource overhead (pool management)

______________________________________________________________________

### Option 4: Combined Approach (Maximum Impact) ‚≠ê RECOMMENDED

**Approach**: Use incremental scanning for file filtering + mahavishnu pools for parallel execution.

**Workflow**:

```
1. File Scanner (git-diff)
   ‚îî‚îÄ> Get 20 changed files

2. Task Distributor
   ‚îî‚îÄ> Split into 4 chunks (5 files each)

3. Mahavishnu Pool (8 workers)
   ‚îú‚îÄ> Worker 1: refurb chunk 1
   ‚îú‚îÄ> Worker 2: refurb chunk 2
   ‚îú‚îÄ> Worker 3: complexipy chunk 1
   ‚îú‚îÄ> Worker 4: complexipy chunk 2
   ‚îú‚îÄ> Worker 5: skylos chunk 1
   ‚îú‚îÄ> Worker 6: skylos chunk 2
   ‚îú‚îÄ> Worker 7: semgrep all files
   ‚îî‚îÄ> Worker 8: gitleaks all files

4. Result Aggregator
   ‚îî‚îÄ> Combine outputs, generate report
```

**Performance**:
| Commit Size | Old Time | New Time | Speedup |
|-------------|----------|----------|---------|
| Small (5-10 files) | 10+ min | 10-20s | **30-60x** |
| Medium (10-50 files) | 10+ min | 20-40s | **15-30x** |
| Large (50+ files) | 10+ min | 2-3 min | **3-5x** |
| Publish (full scan) | 10+ min | 3-4 min | **2.5-3x** |

**Implementation Phases**:

- **Phase 1** (2-3 hours): Git-diff incremental scanning
- **Phase 2** (2-3 hours): Mahavishnu pool integration
- **Phase 3** (1-2 hours): Combine incremental + pools
- **Phase 4** (2-3 hours): Optimization and monitoring

**Total Effort**: 7-11 hours
**Total Speedup**: 30-60x for typical commits
**Risk**: Medium (complex but phased)

______________________________________________________________________

## Recommendations by Use Case

### Daily Development Workflow

**Goal**: \<60 second feedback for typical commits

**Solution**: Option 2C (Hybrid Incremental)

- Use git-diff for changed files
- Run tools only on changes
- Weekly full scan as safety net
- **Time**: 30-60s (vs 10+ min)
- **Effort**: 3-4 hours

### Pre-commit/CI Workflow

**Goal**: 1-3 minutes for comprehensive checks

**Solution**: Option 4 (Combined) - Phase 1 & 2

- Incremental scanning (git-diff)
- Mahavishnu pools (8 workers)
- Run all changed files in parallel
- **Time**: 20-40s (vs 10+ min)
- **Effort**: 4-6 hours

### Publish/Release Workflow

**Goal**: Comprehensive checks in 5-10 minutes

**Solution**: Option 3 (Mahavishnu Pools) + Tool Substitution

- Full repository scan
- Mahavishnu pools for parallel execution
- Use complexipy only for publish (not daily)
- **Time**: 3-4 min (vs 10+ min)
- **Effort**: 2-3 hours (pools) + 1 hour (config)

______________________________________________________________________

## Tool-Specific Recommendations

### refurb (Python Modernization)

**Status**: ‚úÖ Keep using!
**Reason**: Actively maintained, finds useful modernization opportunities for Python 3.13+
**Strategy**:

- Daily: Incremental scan (changed files only)
- Publish: Full scan with mahavishnu pools
  **Time Savings**: 290s ‚Üí 10-30s (incremental), ~100s (pooled)

### complexipy (Cognitive Complexity)

**Status**: ‚ö†Ô∏è Too slow for daily use
**Reason**: 605 seconds is excessive even for publish
**Strategy**:

- Daily: Use ruff complexity instead (10-20x faster)
- Publish: Use complexipy with mahavishnu pools (3-4x faster than current)
  **Time Savings**: 605s ‚Üí 5s (ruff), ~150s (pooled complexipy)

### skylos (Dead Code Detection)

**Status**: ‚úÖ Excellent, but position wrong
**Reason**: Rust-based, fastest dead code detector available
**Strategy**:

- Daily: Use vulture (slower but good enough)
- Publish: Use skylos (fastest and most comprehensive)
  **Time Savings**: 60s ‚Üí 30s (vulture), ~20s (pooled skylos)

______________________________________________________________________

## Implementation Priority

### Quick Wins (1-2 hours, 5-10x speedup)

1. ‚úÖ Switch daily complexity to ruff
1. ‚úÖ Use vulture for daily dead code detection
1. ‚úÖ Keep skylos, refurb, complexipy for publish only

**Impact**: Most workflows see 5-10x speedup immediately
**Risk**: None (tool configuration only)

### High Impact (3-4 hours, 10-20x speedup)

4. ‚úÖ Implement git-diff incremental scanning
1. ‚úÖ Add weekly full scan safety net
1. ‚úÖ Update hook configuration

**Impact**: Typical commits 10-20x faster
**Risk**: Low (proven git-diff approach)

### Maximum Impact (7-11 hours total, 30-60x speedup)

7. ‚úÖ Integrate mahavishnu pools (Phase 1)
1. ‚úÖ Combine incremental + pools (Phase 2)
1. ‚úÖ Add monitoring and optimization (Phase 3)

**Impact**: All workflows 30-60x faster for typical commits
**Risk**: Medium (but mitigated by phased approach)

______________________________________________________________________

## Configuration Examples

### Daily Workflow (Fast)

```yaml
# .crackerjack.yaml.daily

mode: incremental
incremental:
  use_git_diff: true
  fallback_to_full: false

tools:
  fast:
    - ruff           # Formatting, complexity
    - vulture        # Dead code
    - codespell      # Typos
    - refurb         # Modernization (incremental)
```

**Expected Time**: 30-60 seconds for typical commits

### Publish Workflow (Comprehensive)

```yaml
# .crackerjack.yaml.publish

mode: full
pools:
  enabled: true
  workers: 8

tools:
  fast:
    - ruff
    - refurb
    - skylos

  comprehensive:
    - complexipy     # Only for publish
    - semgrep        # Security
    - gitleaks       # Secrets
```

**Expected Time**: 3-4 minutes for full scan

______________________________________________________________________

## Migration Path

### Week 1: Tool Substitution

- [ ] Switch daily workflow to ruff + vulture
- [ ] Keep refurb, skylos, complexipy for publish
- [ ] Update documentation
- [ ] Test with team

### Week 2: Incremental Scanning

- [ ] Implement git-diff scanner
- [ ] Add weekly full scan
- [ ] Update hook runners
- [ ] Monitor cache hit rates

### Week 3: Mahavishnu Integration

- [ ] Restart and connect mahavishnu MCP
- [ ] Implement pool client
- [ ] Add pool-based hooks
- [ ] Benchmark performance

### Week 4: Optimization

- [ ] Combine incremental + pools
- [ ] Add monitoring dashboards
- [ ] Tune worker counts
- [ ] Document best practices

______________________________________________________________________

## Open Questions for Consultants

1. **Tool Strategy**: Do you agree refurb is valuable for Python 3.13+ codebases?
1. **Incremental Approach**: Git-diff (simple) vs marker-based (accurate)?
1. **Full Scan Frequency**: Weekly, bi-weekly, or monthly?
1. **Pool Workers**: How many workers? (4, 8, 16?)
1. **Mahavishnu Availability**: Running in all environments? (local, CI, docker)
1. **Fallback Strategy**: What if mahavishna is unavailable?
1. **Metrics**: What should we track? (scan times, cache hit rates, tool failures?)
1. **Team Workflow**: How often do you publish? (affects full scan frequency)

______________________________________________________________________

## Decision Framework

### Choose Option 1 (Tool Substitution) if:

- ‚úÖ You need immediate results
- ‚úÖ Minimal implementation effort
- ‚úÖ Willing to accept different tool outputs
- ‚ùå Don't need full repository scans

### Choose Option 2 (Incremental Scanning) if:

- ‚úÖ Most commits touch \<50 files
- ‚úÖ Can accept weekly full scans
- ‚úÖ Want simple, proven approach
- ‚ùå Don't have mahavishnu available

### Choose Option 3 (Mahavishnu Pools) if:

- ‚úÖ Large codebase (>100k files)
- ‚úÖ Have mahavishnu infrastructure
- ‚úÖ Need full scans regularly
- ‚ùå Can handle distributed complexity

### Choose Option 4 (Combined) if:

- ‚úÖ Want maximum performance (30-60x faster)
- ‚úÖ Willing to invest 7-11 hours
- ‚úÖ Have mahavishnu or can set it up
- ‚úÖ Want scalable, future-proof solution
- ‚≠ê **RECOMMENDED FOR MOST TEAMS**

______________________________________________________________________

## Next Steps

1. **üîç Review This Document**: Share with team and consultants
1. **üí¨ Discuss Strategy**: Answer open questions, prioritize approach
1. **üöÄ Choose Path**: Select option(s) based on constraints
1. **üìã Implementation**: Follow phased migration path
1. **üìä Measure Success**: Track metrics and optimize

______________________________________________________________________

**Prepared by**: Claude Sonnet 4.5
**Date**: 2026-02-13
**Status**: Ready for decision
**Related Docs**:

- `docs/INTEGRAL_SCANNING_OPTIONS.md` - Incremental scanning details
- `docs/MAHAVISHNU_POOL_INTEGRATION.md` - Pool architecture
