#!/usr/bin/env python3
"""LLM provider management MCP tools.

This module provides tools for managing and interacting with LLM providers
following crackerjack architecture patterns.
"""

from __future__ import annotations

import logging

logger = logging.getLogger(__name__)

# Lazy loading for optional LLM dependencies
_llm_manager = None
_llm_available = None


async def _get_llm_manager():
    """Get LLM manager instance with lazy loading."""
    global _llm_manager, _llm_available

    if _llm_available is False:
        return None

    if _llm_manager is None:
        try:
            from session_mgmt_mcp.llm_providers import LLMManager

            _llm_manager = LLMManager()
            _llm_available = True
        except ImportError as e:
            logger.warning(f"LLM providers not available: {e}")
            _llm_available = False
            return None

    return _llm_manager


def _check_llm_available() -> bool:
    """Check if LLM providers are available."""
    global _llm_available

    if _llm_available is None:
        try:
            import importlib.util

            spec = importlib.util.find_spec("session_mgmt_mcp.llm_providers")
            _llm_available = spec is not None
        except ImportError:
            _llm_available = False

    return _llm_available


def register_llm_tools(mcp) -> None:
    """Register all LLM provider management MCP tools.

    Args:
        mcp: FastMCP server instance

    """

    @mcp.tool()
    async def list_llm_providers() -> str:
        """List all available LLM providers and their models."""
        if not _check_llm_available():
            return "âŒ LLM providers not available. Install dependencies: pip install openai google-generativeai aiohttp"

        try:
            manager = await _get_llm_manager()
            if not manager:
                return "âŒ Failed to initialize LLM manager"

            available_providers = await manager.get_available_providers()
            provider_info = manager.get_provider_info()

            output = ["ğŸ¤– Available LLM Providers", ""]

            for provider_name, info in provider_info["providers"].items():
                status = "âœ…" if provider_name in available_providers else "âŒ"
                output.append(f"{status} {provider_name.title()}")

                if provider_name in available_providers:
                    models = info["models"][:5]  # Show first 5 models
                    for model in models:
                        output.append(f"   â€¢ {model}")
                    if len(info["models"]) > 5:
                        output.append(f"   â€¢ ... and {len(info['models']) - 5} more")
                output.append("")

            config = provider_info["config"]
            output.extend(
                [
                    f"ğŸ¯ Default Provider: {config['default_provider']}",
                    f"ğŸ”„ Fallback Providers: {', '.join(config['fallback_providers'])}",
                ]
            )

            return "\n".join(output)

        except Exception as e:
            logger.exception("Error listing LLM providers", error=str(e))
            return f"âŒ Error listing providers: {e}"

    @mcp.tool()
    async def test_llm_providers() -> str:
        """Test all LLM providers to check their availability and functionality."""
        if not _check_llm_available():
            return "âŒ LLM providers not available. Install dependencies: pip install openai google-generativeai aiohttp"

        try:
            manager = await _get_llm_manager()
            if not manager:
                return "âŒ Failed to initialize LLM manager"

            test_results = await manager.test_all_providers()

            output = ["ğŸ§ª LLM Provider Test Results", ""]

            for provider, result in test_results.items():
                status = "âœ…" if result["success"] else "âŒ"
                output.append(f"{status} {provider.title()}")

                if result["success"]:
                    output.append(
                        f"   âš¡ Response time: {result['response_time_ms']:.0f}ms"
                    )
                    output.append(f"   ğŸ¯ Model: {result['model']}")
                else:
                    output.append(f"   âŒ Error: {result['error']}")
                output.append("")

            working_count = sum(1 for r in test_results.values() if r["success"])
            total_count = len(test_results)
            output.append(
                f"ğŸ“Š Summary: {working_count}/{total_count} providers working"
            )

            return "\n".join(output)

        except Exception as e:
            logger.exception("Error testing LLM providers", error=str(e))
            return f"âŒ Error testing providers: {e}"

    @mcp.tool()
    async def generate_with_llm(
        prompt: str,
        provider: str | None = None,
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        use_fallback: bool = True,
    ) -> str:
        """Generate text using specified LLM provider.

        Args:
            prompt: The text prompt to generate from
            provider: LLM provider to use (openai, gemini, ollama)
            model: Specific model to use
            temperature: Generation temperature (0.0-1.0)
            max_tokens: Maximum tokens to generate
            use_fallback: Whether to use fallback providers if primary fails

        """
        if not _check_llm_available():
            return "âŒ LLM providers not available. Install dependencies: pip install openai google-generativeai aiohttp"

        try:
            manager = await _get_llm_manager()
            if not manager:
                return "âŒ Failed to initialize LLM manager"

            result = await manager.generate_text(
                prompt=prompt,
                provider=provider,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                use_fallback=use_fallback,
            )

            if result["success"]:
                output = ["âœ¨ LLM Generation Result", ""]
                output.append(f"ğŸ¤– Provider: {result['metadata']['provider']}")
                output.append(f"ğŸ¯ Model: {result['metadata']['model']}")
                output.append(
                    f"âš¡ Response time: {result['metadata']['response_time_ms']:.0f}ms"
                )
                output.append(
                    f"ğŸ“Š Tokens: {result['metadata'].get('tokens_used', 'N/A')}"
                )
                output.append("")
                output.append("ğŸ’¬ Generated text:")
                output.append("â”€" * 40)
                output.append(result["text"])

                return "\n".join(output)
            return f"âŒ Generation failed: {result['error']}"

        except Exception as e:
            logger.exception("Error generating with LLM", error=str(e))
            return f"âŒ Error generating text: {e}"

    @mcp.tool()
    async def chat_with_llm(
        messages: list[dict[str, str]],
        provider: str | None = None,
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int | None = None,
    ) -> str:
        """Have a conversation with an LLM provider.

        Args:
            messages: List of messages in format [{"role": "user/assistant/system", "content": "text"}]
            provider: LLM provider to use (openai, gemini, ollama)
            model: Specific model to use
            temperature: Generation temperature (0.0-1.0)
            max_tokens: Maximum tokens to generate

        """
        if not _check_llm_available():
            return "âŒ LLM providers not available. Install dependencies: pip install openai google-generativeai aiohttp"

        try:
            manager = await _get_llm_manager()
            if not manager:
                return "âŒ Failed to initialize LLM manager"

            result = await manager.chat(
                messages=messages,
                provider=provider,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
            )

            if result["success"]:
                output = ["ğŸ’¬ LLM Chat Result", ""]
                output.append(f"ğŸ¤– Provider: {result['metadata']['provider']}")
                output.append(f"ğŸ¯ Model: {result['metadata']['model']}")
                output.append(
                    f"âš¡ Response time: {result['metadata']['response_time_ms']:.0f}ms"
                )
                output.append(f"ğŸ“Š Messages: {len(messages)} â†’ 1")
                output.append("")
                output.append("ğŸ­ Assistant response:")
                output.append("â”€" * 40)
                output.append(result["response"])

                return "\n".join(output)
            return f"âŒ Chat failed: {result['error']}"

        except Exception as e:
            logger.exception("Error chatting with LLM", error=str(e))
            return f"âŒ Error in chat: {e}"

    @mcp.tool()
    async def configure_llm_provider(
        provider: str,
        api_key: str | None = None,
        base_url: str | None = None,
        default_model: str | None = None,
    ) -> str:
        """Configure an LLM provider with API credentials and settings.

        Args:
            provider: Provider name (openai, gemini, ollama)
            api_key: API key for the provider
            base_url: Base URL for the provider API
            default_model: Default model to use

        """
        if not _check_llm_available():
            return "âŒ LLM providers not available. Install dependencies: pip install openai google-generativeai aiohttp"

        try:
            manager = await _get_llm_manager()
            if not manager:
                return "âŒ Failed to initialize LLM manager"

            config_data = {}
            if api_key:
                config_data["api_key"] = api_key
            if base_url:
                config_data["base_url"] = base_url
            if default_model:
                config_data["default_model"] = default_model

            result = await manager.configure_provider(provider, config_data)

            if result["success"]:
                output = ["âš™ï¸ Provider Configuration Updated", ""]
                output.append(f"ğŸ¤– Provider: {provider}")

                if api_key:
                    # Don't show the full API key for security
                    masked_key = (
                        api_key[:8] + "..." + api_key[-4:]
                        if len(api_key) > 12
                        else "***"
                    )
                    output.append(f"ğŸ”‘ API Key: {masked_key}")

                if base_url:
                    output.append(f"ğŸŒ Base URL: {base_url}")

                if default_model:
                    output.append(f"ğŸ¯ Default Model: {default_model}")

                output.append("")
                output.append("âœ… Configuration saved successfully!")
                output.append("ğŸ’¡ Use `test_llm_providers` to verify the configuration")

                return "\n".join(output)
            return f"âŒ Configuration failed: {result['error']}"

        except Exception as e:
            logger.exception("Error configuring LLM provider", error=str(e))
            return f"âŒ Error configuring provider: {e}"
