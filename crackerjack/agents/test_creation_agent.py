import ast
import json
from pathlib import Path
from typing import Any

from .base import (
    AgentContext,
    FixResult,
    Issue,
    IssueType,
    SubAgent,
    agent_registry,
)


class TestCreationAgent(SubAgent):
    def __init__(self, context: AgentContext) -> None:
        super().__init__(context)

    def get_supported_types(self) -> set[IssueType]:
        return {
            IssueType.TEST_FAILURE,
            IssueType.DEPENDENCY,
            IssueType.TEST_ORGANIZATION,
            IssueType.COVERAGE_IMPROVEMENT,
        }

    async def can_handle(self, issue: Issue) -> float:
        """Enhanced confidence scoring based on issue complexity and expected impact."""
        if issue.type not in self.get_supported_types():
            return 0.0

        message_lower = issue.message.lower()

        # High confidence for coverage improvement - key audit requirement
        if issue.type == IssueType.COVERAGE_IMPROVEMENT:
            # Check for specific coverage improvement scenarios
            if any(
                term in message_lower
                for term in [
                    "coverage below",
                    "missing tests",
                    "untested functions",
                    "no tests found",
                    "coverage requirement",
                ]
            ):
                return 0.95  # Enhanced confidence for coverage issues
            return 0.9

        if issue.type == IssueType.TEST_ORGANIZATION:
            return self._check_test_organization_confidence(message_lower)

        # Enhanced pattern matching for test creation needs
        perfect_score = self._check_perfect_test_creation_matches(message_lower)
        if perfect_score > 0:
            return perfect_score

        good_score = self._check_good_test_creation_matches(message_lower)
        if good_score > 0:
            return good_score

        # Improved file path analysis
        file_path_score = self._check_file_path_test_indicators(issue.file_path)
        if file_path_score > 0:
            return file_path_score

        # New: Check for untested functions specifically
        if self._indicates_untested_functions(message_lower):
            return 0.85

        return 0.0

    def _check_test_organization_confidence(self, message_lower: str) -> float:
        organization_keywords = [
            "redundant tests",
            "duplicate tests",
            "overlapping tests",
            "consolidate tests",
            "test suite optimization",
            "obsolete tests",
            "broken tests",
            "coverage booster",
            "victory test",
            "test cleanup",
        ]
        return (
            0.9
            if any(keyword in message_lower for keyword in organization_keywords)
            else 0.7
        )

    def _check_perfect_test_creation_matches(self, message_lower: str) -> float:
        perfect_keywords = [
            "coverage below",
            "missing tests",
            "untested",
            "no tests found",
            "test coverage",
            "coverage requirement",
            "coverage gap",
        ]
        return (
            1.0
            if any(keyword in message_lower for keyword in perfect_keywords)
            else 0.0
        )

    def _check_good_test_creation_matches(self, message_lower: str) -> float:
        good_keywords = [
            "coverage",
            "test",
            "missing",
            "untested code",
            "no test",
            "empty test",
            "test missing",
        ]
        return (
            0.8 if any(keyword in message_lower for keyword in good_keywords) else 0.0
        )

    def _check_file_path_test_indicators(self, file_path: str | None) -> float:
        if not file_path:
            return 0.0

        # Enhanced file path analysis
        if not self._has_corresponding_test(file_path):
            # Higher confidence for core modules
            if any(
                core_path in file_path
                for core_path in ["/managers/", "/services/", "/core/", "/agents/"]
            ):
                return 0.8
            return 0.7
        return 0.0

    def _indicates_untested_functions(self, message_lower: str) -> bool:
        """Check if message indicates untested functions."""
        return any(
            indicator in message_lower
            for indicator in [
                "function not tested",
                "untested method",
                "no test for function",
                "function coverage",
                "method coverage",
                "untested code path",
            ]
        )

    async def analyze_and_fix(self, issue: Issue) -> FixResult:
        self.log(f"Analyzing test creation need: {issue.message}")

        try:
            fixes_applied, files_modified = await self._apply_test_creation_fixes(issue)
            return self._create_test_creation_result(fixes_applied, files_modified)

        except Exception as e:
            self.log(f"Error creating tests: {e}", "ERROR")
            return self._create_error_result(e)

    async def _apply_test_creation_fixes(
        self,
        issue: Issue,
    ) -> tuple[list[str], list[str]]:
        fixes_applied: list[str] = []
        files_modified: list[str] = []

        coverage_fixes, coverage_files = await self._apply_coverage_based_fixes()
        fixes_applied.extend(coverage_fixes)
        files_modified.extend(coverage_files)

        file_fixes, file_modified = await self._apply_file_specific_fixes(
            issue.file_path,
        )
        fixes_applied.extend(file_fixes)
        files_modified.extend(file_modified)

        function_fixes, function_files = await self._apply_function_specific_fixes()
        fixes_applied.extend(function_fixes)
        files_modified.extend(function_files)

        return fixes_applied, files_modified

    async def _apply_coverage_based_fixes(self) -> tuple[list[str], list[str]]:
        fixes_applied: list[str] = []
        files_modified: list[str] = []

        coverage_analysis = await self._analyze_coverage()

        if coverage_analysis["below_threshold"]:
            self.log(
                f"Coverage below threshold: {coverage_analysis['current_coverage']:.1%}",
            )

            for module_path in coverage_analysis["uncovered_modules"]:
                test_fixes = await self._create_tests_for_module(module_path)
                fixes_applied.extend(test_fixes["fixes"])
                files_modified.extend(test_fixes["files"])

        return fixes_applied, files_modified

    async def _apply_file_specific_fixes(
        self,
        file_path: str | None,
    ) -> tuple[list[str], list[str]]:
        if not file_path:
            return [], []

        file_fixes = await self._create_tests_for_file(file_path)
        return file_fixes["fixes"], file_fixes["files"]

    async def _apply_function_specific_fixes(self) -> tuple[list[str], list[str]]:
        fixes_applied: list[str] = []
        files_modified: list[str] = []

        untested_functions = await self._find_untested_functions()
        for func_info in untested_functions[:5]:
            func_fixes = await self._create_test_for_function(func_info)
            fixes_applied.extend(func_fixes["fixes"])
            files_modified.extend(func_fixes["files"])

        return fixes_applied, files_modified

    def _create_test_creation_result(
        self,
        fixes_applied: list[str],
        files_modified: list[str],
    ) -> FixResult:
        """Enhanced result creation with detailed confidence scoring."""
        success = len(fixes_applied) > 0

        # Enhanced confidence calculation based on types of fixes applied
        confidence = 0.5  # Base confidence

        if success:
            # Higher confidence based on quality of fixes
            test_file_fixes = [f for f in fixes_applied if "test file" in f.lower()]
            function_fixes = [f for f in fixes_applied if "function" in f.lower()]
            coverage_fixes = [f for f in fixes_applied if "coverage" in f.lower()]

            # Boost confidence for comprehensive test creation
            if test_file_fixes:
                confidence += 0.25  # Test file creation
            if function_fixes:
                confidence += 0.15  # Function-specific tests
            if coverage_fixes:
                confidence += 0.1  # Coverage improvements

            # Additional boost for multiple file creation (broader impact)
            if len(files_modified) > 2:
                confidence += 0.1

            # Cap at 0.95 to maintain some uncertainty
            confidence = min(confidence, 0.95)

        recommendations = (
            [] if success else self._get_enhanced_test_creation_recommendations()
        )

        return FixResult(
            success=success,
            confidence=confidence,
            fixes_applied=fixes_applied,
            files_modified=files_modified,
            recommendations=recommendations,
        )

    def _get_enhanced_test_creation_recommendations(self) -> list[str]:
        """Enhanced recommendations based on audit requirements."""
        return [
            "Run 'python -m crackerjack -t' to execute comprehensive coverage analysis",
            "Focus on testing high-priority functions in managers/ services/ and core/ modules",
            "Implement parametrized tests (@pytest.mark.parametrize) for functions with multiple arguments",
            "Add edge case testing for boundary conditions and error scenarios",
            "Use fixtures for complex object instantiation and dependency injection",
            "Consider integration tests for modules with multiple classes/functions",
            "Add async tests for coroutine functions using @pytest.mark.asyncio",
            "Mock external dependencies to ensure isolated unit testing",
            "Target â‰¥10% coverage improvement through systematic test creation",
            "Validate generated tests are syntactically correct before committing",
        ]

    def _create_error_result(self, error: Exception) -> FixResult:
        return FixResult(
            success=False,
            confidence=0.0,
            remaining_issues=[f"Failed to create tests: {error}"],
            recommendations=[
                "Manual test creation may be required",
                "Check existing test structure and patterns",
            ],
        )

    async def _analyze_coverage(self) -> dict[str, Any]:
        """Enhanced coverage analysis with detailed metrics and improvement tracking."""
        try:
            # First try to get coverage from existing reports
            coverage_data = await self._get_existing_coverage_data()
            if coverage_data:
                return coverage_data

            # Run coverage analysis if no existing data
            returncode, stdout, stderr = await self._run_coverage_command()

            if returncode != 0:
                return self._handle_coverage_command_failure(stderr)

            return await self._process_coverage_results_enhanced()

        except Exception as e:
            self.log(f"Coverage analysis error: {e}", "WARN")
            return self._create_default_coverage_result()

    async def _get_existing_coverage_data(self) -> dict[str, Any] | None:
        """Try to get coverage data from existing coverage reports."""
        try:
            # Check for JSON coverage report
            json_report = self.context.project_path / "coverage.json"
            if json_report.exists():
                content = self.context.get_file_content(json_report)
                if content:
                    coverage_json = json.loads(content)
                    return self._parse_coverage_json(coverage_json)

            # Check for .coverage file
            coverage_file = self.context.project_path / ".coverage"
            if coverage_file.exists():
                return await self._process_coverage_results_enhanced()

        except Exception as e:
            self.log(f"Error reading existing coverage: {e}", "WARN")

        return None

    def _parse_coverage_json(self, coverage_json: dict) -> dict[str, Any]:
        """Parse coverage JSON data into our format."""
        try:
            totals = coverage_json.get("totals", {})
            current_coverage = totals.get("percent_covered", 0) / 100.0

            # Find uncovered modules
            uncovered_modules = []
            files = coverage_json.get("files", {})

            for file_path, file_data in files.items():
                if file_data.get("summary", {}).get("percent_covered", 100) < 80:
                    # Convert absolute path to relative
                    rel_path = str(
                        Path(file_path).relative_to(self.context.project_path)
                    )
                    uncovered_modules.append(rel_path)

            return {
                "below_threshold": current_coverage < 0.8,  # 80% threshold
                "current_coverage": current_coverage,
                "uncovered_modules": uncovered_modules[:15],  # Limit for performance
                "missing_lines": totals.get("num_statements", 0)
                - totals.get("covered_lines", 0),
                "total_lines": totals.get("num_statements", 0),
            }

        except Exception as e:
            self.log(f"Error parsing coverage JSON: {e}", "WARN")
            return self._create_default_coverage_result()

    async def _run_coverage_command(self) -> tuple[int, str, str]:
        return await self.run_command(
            [
                "uv",
                "run",
                "python",
                "-m",
                "pytest",
                "--cov=crackerjack",
                "--cov-report=json",
                "-q",
            ],
        )

    def _handle_coverage_command_failure(self, stderr: str) -> dict[str, Any]:
        self.log(f"Coverage analysis failed: {stderr}", "WARN")
        return self._create_default_coverage_result()

    async def _process_coverage_results_enhanced(self) -> dict[str, Any]:
        """Enhanced coverage results processing with detailed analysis."""
        coverage_file = self.context.project_path / ".coverage"
        if not coverage_file.exists():
            return self._create_default_coverage_result()

        # Get more detailed coverage analysis
        uncovered_modules = await self._find_uncovered_modules_enhanced()
        untested_functions = await self._find_untested_functions_enhanced()

        # Estimate current coverage more accurately
        current_coverage = await self._estimate_current_coverage()

        return {
            "below_threshold": current_coverage < 0.8,  # 80% threshold
            "current_coverage": current_coverage,
            "uncovered_modules": uncovered_modules[:15],  # Performance limit
            "untested_functions": untested_functions[:20],  # Top priority functions
            "coverage_gaps": await self._identify_coverage_gaps(),
            "improvement_potential": self._calculate_improvement_potential(
                len(uncovered_modules), len(untested_functions)
            ),
        }

    async def _estimate_current_coverage(self) -> float:
        """Estimate current coverage by analyzing test files vs source files."""
        try:
            source_files = list(
                (self.context.project_path / "crackerjack").rglob("*.py")
            )
            source_files = [f for f in source_files if not f.name.startswith("test_")]

            test_files = list((self.context.project_path / "tests").rglob("test_*.py"))

            if not source_files:
                return 0.0

            # Simple heuristic: ratio of test files to source files
            coverage_ratio = len(test_files) / len(source_files)

            # Adjust based on known coverage patterns
            estimated_coverage = min(coverage_ratio * 0.6, 0.9)  # Cap at 90%

            return estimated_coverage

        except Exception:
            return 0.1  # Conservative estimate

    def _calculate_improvement_potential(
        self, uncovered_modules: int, untested_functions: int
    ) -> dict[str, Any]:
        """Calculate potential coverage improvement from test generation."""
        if uncovered_modules == 0 and untested_functions == 0:
            return {"percentage_points": 0, "priority": "low"}

        # Estimate improvement potential
        module_improvement = uncovered_modules * 2.5  # Each module ~2.5% coverage
        function_improvement = untested_functions * 0.8  # Each function ~0.8% coverage

        total_potential = min(
            module_improvement + function_improvement, 40
        )  # Cap at 40%

        priority = (
            "high"
            if total_potential > 15
            else "medium"
            if total_potential > 5
            else "low"
        )

        return {
            "percentage_points": round(total_potential, 1),
            "priority": priority,
            "module_contribution": round(module_improvement, 1),
            "function_contribution": round(function_improvement, 1),
        }

    def _create_default_coverage_result(self) -> dict[str, Any]:
        return {
            "below_threshold": True,
            "current_coverage": 0.0,
            "uncovered_modules": [],
        }

    async def _find_uncovered_modules_enhanced(self) -> list[dict[str, Any]]:
        """Enhanced uncovered modules detection with priority scoring."""
        uncovered: list[dict[str, Any]] = []

        package_dir = self.context.project_path / "crackerjack"
        if not package_dir.exists():
            return uncovered[:15]

        for py_file in package_dir.rglob("*.py"):
            if self._should_skip_module_for_coverage(py_file):
                continue

            if not self._has_corresponding_test(str(py_file)):
                module_info = await self._analyze_module_priority(py_file)
                uncovered.append(module_info)

        # Sort by priority (highest first)
        uncovered.sort(key=lambda x: x["priority_score"], reverse=True)
        return uncovered[:15]

    async def _analyze_module_priority(self, py_file: Path) -> dict[str, Any]:
        """Analyze module to determine testing priority."""
        try:
            content = self.context.get_file_content(py_file) or ""
            ast.parse(content)

            functions = await self._extract_functions_from_file(py_file)
            classes = await self._extract_classes_from_file(py_file)

            # Calculate priority score
            priority_score = 0

            # Core modules get higher priority
            rel_path = str(py_file.relative_to(self.context.project_path))
            if any(
                core_path in rel_path
                for core_path in ["managers/", "services/", "core/", "agents/"]
            ):
                priority_score += 10

            # More functions/classes = higher priority
            priority_score += len(functions) * 2
            priority_score += len(classes) * 3

            # Public API functions get higher priority
            public_functions = [f for f in functions if not f["name"].startswith("_")]
            priority_score += len(public_functions) * 2

            # File size consideration (larger files need tests more)
            lines_count = len(content.split("\n"))
            if lines_count > 100:
                priority_score += 5
            elif lines_count > 50:
                priority_score += 2

            return {
                "path": rel_path,
                "absolute_path": str(py_file),
                "priority_score": priority_score,
                "function_count": len(functions),
                "class_count": len(classes),
                "public_function_count": len(public_functions),
                "lines_count": lines_count,
                "category": self._categorize_module(rel_path),
            }

        except Exception as e:
            self.log(f"Error analyzing module priority for {py_file}: {e}", "WARN")
            return {
                "path": str(py_file.relative_to(self.context.project_path)),
                "absolute_path": str(py_file),
                "priority_score": 1,
                "function_count": 0,
                "class_count": 0,
                "public_function_count": 0,
                "lines_count": 0,
                "category": "unknown",
            }

    def _categorize_module(self, relative_path: str) -> str:
        """Categorize module for test generation strategies."""
        if "managers/" in relative_path:
            return "manager"
        elif "services/" in relative_path:
            return "service"
        elif "core/" in relative_path:
            return "core"
        elif "agents/" in relative_path:
            return "agent"
        elif "models/" in relative_path:
            return "model"
        elif "executors/" in relative_path:
            return "executor"
        else:
            return "utility"

    async def _find_untested_functions_enhanced(self) -> list[dict[str, Any]]:
        """Enhanced untested function detection with detailed analysis."""
        untested: list[dict[str, Any]] = []

        package_dir = self.context.project_path / "crackerjack"
        if not package_dir.exists():
            return untested[:20]

        for py_file in package_dir.rglob("*.py"):
            if self._should_skip_file_for_testing(py_file):
                continue

            file_untested = await self._find_untested_functions_in_file_enhanced(
                py_file
            )
            untested.extend(file_untested)

        # Sort by testing priority
        untested.sort(key=lambda x: x["testing_priority"], reverse=True)
        return untested[:20]

    async def _find_untested_functions_in_file_enhanced(
        self, py_file: Path
    ) -> list[dict[str, Any]]:
        """Enhanced untested function detection with priority scoring."""
        untested: list[dict[str, Any]] = []

        try:
            functions = await self._extract_functions_from_file(py_file)
            for func in functions:
                if not await self._function_has_test(func, py_file):
                    func_info = await self._analyze_function_testability(func, py_file)
                    untested.append(func_info)

        except Exception as e:
            self.log(f"Error finding untested functions in {py_file}: {e}", "WARN")

        return untested

    async def _analyze_function_testability(
        self, func: dict[str, Any], py_file: Path
    ) -> dict[str, Any]:
        """Analyze function to determine testing priority and approach."""
        try:
            # Basic function info
            func_info = {
                "name": func["name"],
                "file": str(py_file),
                "relative_file": str(py_file.relative_to(self.context.project_path)),
                "line": func.get("line", 1),
                "signature": func.get("signature", ""),
                "args": func.get("args", []),
                "returns": func.get("returns", "Any"),
                "testing_priority": 0,
                "complexity": "simple",
                "test_strategy": "basic",
            }

            # Calculate testing priority
            priority = 0

            # Public functions get higher priority
            if not func["name"].startswith("_"):
                priority += 10

            # Functions with multiple args are more complex
            arg_count = len(func.get("args", []))
            if arg_count > 3:
                priority += 5
                func_info["complexity"] = "complex"
                func_info["test_strategy"] = "parametrized"
            elif arg_count > 1:
                priority += 2
                func_info["complexity"] = "moderate"

            # Core module functions get higher priority
            if any(
                core_path in func_info["relative_file"]
                for core_path in ["managers/", "services/", "core/"]
            ):
                priority += 8

            # Async functions need special handling
            if func.get("is_async", False):
                priority += 3
                func_info["test_strategy"] = "async"

            func_info["testing_priority"] = priority

            return func_info

        except Exception as e:
            self.log(f"Error analyzing function testability: {e}", "WARN")
            return {
                "name": func.get("name", "unknown"),
                "file": str(py_file),
                "relative_file": str(py_file.relative_to(self.context.project_path)),
                "line": func.get("line", 1),
                "testing_priority": 1,
                "complexity": "unknown",
                "test_strategy": "basic",
            }

    async def _identify_coverage_gaps(self) -> list[dict[str, Any]]:
        """Identify specific coverage gaps that can be addressed."""
        gaps = []

        try:
            # Find modules with partial test coverage
            package_dir = self.context.project_path / "crackerjack"
            tests_dir = self.context.project_path / "tests"

            if not package_dir.exists() or not tests_dir.exists():
                return gaps

            for py_file in package_dir.rglob("*.py"):
                if self._should_skip_module_for_coverage(py_file):
                    continue

                test_coverage_info = await self._analyze_existing_test_coverage(py_file)
                if test_coverage_info["has_gaps"]:
                    gaps.append(test_coverage_info)

        except Exception as e:
            self.log(f"Error identifying coverage gaps: {e}", "WARN")

        return gaps[:10]  # Limit for performance

    async def _analyze_existing_test_coverage(self, py_file: Path) -> dict[str, Any]:
        """Analyze existing test coverage for a specific file."""
        try:
            test_file_path = await self._generate_test_file_path(py_file)

            coverage_info = {
                "source_file": str(py_file.relative_to(self.context.project_path)),
                "test_file": str(test_file_path) if test_file_path.exists() else None,
                "has_gaps": True,  # Default assumption
                "missing_test_types": [],
                "coverage_score": 0,
            }

            if not test_file_path.exists():
                coverage_info["missing_test_types"] = [
                    "basic",
                    "edge_cases",
                    "error_handling",
                ]
                return coverage_info

            # Analyze existing test file
            test_content = self.context.get_file_content(test_file_path) or ""

            # Check for different test types
            missing_types = []
            if "def test_" not in test_content:
                missing_types.append("basic")
            if "@pytest.mark.parametrize" not in test_content:
                missing_types.append("parametrized")
            if "with pytest.raises" not in test_content:
                missing_types.append("error_handling")
            if "mock" not in test_content.lower():
                missing_types.append("mocking")

            coverage_info["missing_test_types"] = missing_types
            coverage_info["has_gaps"] = len(missing_types) > 0
            coverage_info["coverage_score"] = max(0, 100 - len(missing_types) * 25)

            return coverage_info

        except Exception as e:
            self.log(f"Error analyzing test coverage for {py_file}: {e}", "WARN")
            return {
                "source_file": str(py_file.relative_to(self.context.project_path)),
                "test_file": None,
                "has_gaps": True,
                "missing_test_types": ["basic"],
                "coverage_score": 0,
            }

    def _should_skip_module_for_coverage(self, py_file: Path) -> bool:
        return py_file.name.startswith("test_") or py_file.name == "__init__.py"

    def _get_relative_module_path(self, py_file: Path) -> str:
        return str(py_file.relative_to(self.context.project_path))

    def _has_corresponding_test(self, file_path: str) -> bool:
        path = Path(file_path)

        test_patterns = [
            f"test_{path.stem}.py",
            f"{path.stem}_test.py",
            f"test_{path.stem}_*.py",
        ]

        tests_dir = self.context.project_path / "tests"
        if tests_dir.exists():
            for pattern in test_patterns:
                if list(tests_dir.glob(pattern)):
                    return True

        return False

    async def _create_tests_for_module(self, module_path: str) -> dict[str, list[str]]:
        fixes: list[str] = []
        files: list[str] = []

        try:
            module_file = Path(module_path)
            if not module_file.exists():
                return {"fixes": fixes, "files": files}

            functions = await self._extract_functions_from_file(module_file)
            classes = await self._extract_classes_from_file(module_file)

            if not functions and not classes:
                return {"fixes": fixes, "files": files}

            test_file_path = await self._generate_test_file_path(module_file)
            test_content = await self._generate_test_content(
                module_file,
                functions,
                classes,
            )

            if self.context.write_file_content(test_file_path, test_content):
                fixes.append(f"Created test file for {module_path}")
                files.append(str(test_file_path))
                self.log(f"Created test file: {test_file_path}")

        except Exception as e:
            self.log(f"Error creating tests for module {module_path}: {e}", "ERROR")

        return {"fixes": fixes, "files": files}

    async def _create_tests_for_file(self, file_path: str) -> dict[str, list[str]]:
        if self._has_corresponding_test(file_path):
            return {"fixes": [], "files": []}

        return await self._create_tests_for_module(file_path)

    async def _find_untested_functions(self) -> list[dict[str, Any]]:
        untested: list[dict[str, Any]] = []

        package_dir = self.context.project_path / "crackerjack"
        if not package_dir.exists():
            return untested[:10]

        for py_file in package_dir.rglob("*.py"):
            if self._should_skip_file_for_testing(py_file):
                continue

            file_untested = await self._find_untested_functions_in_file(py_file)
            untested.extend(file_untested)

        return untested[:10]

    def _should_skip_file_for_testing(self, py_file: Path) -> bool:
        return py_file.name.startswith("test_")

    async def _find_untested_functions_in_file(
        self,
        py_file: Path,
    ) -> list[dict[str, Any]]:
        untested: list[dict[str, Any]] = []

        functions = await self._extract_functions_from_file(py_file)
        for func in functions:
            if not await self._function_has_test(func, py_file):
                untested.append(self._create_untested_function_info(func, py_file))

        return untested

    def _create_untested_function_info(
        self,
        func: dict[str, Any],
        py_file: Path,
    ) -> dict[str, Any]:
        return {
            "name": func["name"],
            "file": str(py_file),
            "line": func.get("line", 1),
            "signature": func.get("signature", ""),
        }

    async def _create_test_for_function(
        self,
        func_info: dict[str, Any],
    ) -> dict[str, list[str]]:
        fixes: list[str] = []
        files: list[str] = []

        try:
            func_file = Path(func_info["file"])
            test_file_path = await self._generate_test_file_path(func_file)

            if test_file_path.exists():
                existing_content = self.context.get_file_content(test_file_path) or ""
                new_test = await self._generate_function_test(func_info)

                updated_content = existing_content.rstrip() + "\n\n" + new_test
                if self.context.write_file_content(test_file_path, updated_content):
                    fixes.append(f"Added test for function {func_info['name']}")
                    files.append(str(test_file_path))
            else:
                test_content = await self._generate_minimal_test_file(func_info)
                if self.context.write_file_content(test_file_path, test_content):
                    fixes.append(f"Created test file with test for {func_info['name']}")
                    files.append(str(test_file_path))

        except Exception as e:
            self.log(
                f"Error creating test for function {func_info['name']}: {e}",
                "ERROR",
            )

        return {"fixes": fixes, "files": files}

    async def _extract_functions_from_file(
        self,
        file_path: Path,
    ) -> list[dict[str, Any]]:
        functions = []

        try:
            content = self.context.get_file_content(file_path)
            if not content:
                return functions

            tree = ast.parse(content)
            functions = self._parse_function_nodes(tree)

        except Exception as e:
            self.log(f"Error parsing file {file_path}: {e}", "WARN")

        return functions

    def _parse_function_nodes(self, tree: ast.AST) -> list[dict[str, Any]]:
        """Enhanced function parsing with async function support."""
        functions: list[dict[str, Any]] = []

        for node in ast.walk(tree):
            if isinstance(
                node, ast.FunctionDef | ast.AsyncFunctionDef
            ) and self._is_valid_function_node(node):
                function_info = self._create_function_info(node)
                # Add async detection
                function_info["is_async"] = isinstance(node, ast.AsyncFunctionDef)
                functions.append(function_info)

        return functions

    def _is_valid_function_node(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> bool:
        """Enhanced validation for both sync and async functions."""
        return not node.name.startswith(("_", "test_"))

    def _create_function_info(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> dict[str, Any]:
        """Enhanced function info creation with async support."""
        return {
            "name": node.name,
            "line": node.lineno,
            "signature": self._get_function_signature(node),
            "args": [arg.arg for arg in node.args.args],
            "returns": self._get_return_annotation(node),
            "is_async": isinstance(node, ast.AsyncFunctionDef),
            "docstring": ast.get_docstring(node) or "",
        }

    async def _extract_classes_from_file(self, file_path: Path) -> list[dict[str, Any]]:
        classes = []

        try:
            content = self.context.get_file_content(file_path)
            if not content:
                return classes

            tree = ast.parse(content)
            classes = self._process_ast_nodes_for_classes(tree)

        except Exception as e:
            self.log(f"Error parsing classes from {file_path}: {e}", "WARN")

        return classes

    def _process_ast_nodes_for_classes(self, tree: ast.AST) -> list[dict[str, Any]]:
        classes: list[dict[str, Any]] = []

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and self._should_include_class(node):
                class_info = self._create_class_info(node)
                classes.append(class_info)

        return classes

    def _should_include_class(self, node: ast.ClassDef) -> bool:
        return not node.name.startswith("_")

    def _create_class_info(self, node: ast.ClassDef) -> dict[str, Any]:
        methods = self._extract_public_methods_from_class(node)
        return {"name": node.name, "line": node.lineno, "methods": methods}

    def _extract_public_methods_from_class(self, node: ast.ClassDef) -> list[str]:
        return [
            item.name
            for item in node.body
            if isinstance(item, ast.FunctionDef) and not item.name.startswith("_")
        ]

    def _get_function_signature(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> str:
        """Enhanced function signature generation with async support."""
        args = [arg.arg for arg in node.args.args]
        prefix = "async " if isinstance(node, ast.AsyncFunctionDef) else ""
        return f"{prefix}{node.name}({', '.join(args)})"

    def _get_return_annotation(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> str:
        """Enhanced return annotation extraction with async support."""
        if node.returns:
            return ast.unparse(node.returns) if hasattr(ast, "unparse") else "Any"
        return "Any"

    async def _function_has_test(
        self,
        func_info: dict[str, Any],
        file_path: Path,
    ) -> bool:
        test_file_path = await self._generate_test_file_path(file_path)

        if not test_file_path.exists():
            return False

        test_content = self.context.get_file_content(test_file_path)
        if not test_content:
            return False

        test_patterns = [
            f"test_{func_info['name']}",
            f"test_{func_info['name']}_",
            f"def test_{func_info['name']}",
        ]

        return any(pattern in test_content for pattern in test_patterns)

    async def _generate_test_file_path(self, source_file: Path) -> Path:
        tests_dir = self.context.project_path / "tests"
        tests_dir.mkdir(exist_ok=True)

        relative_path = source_file.relative_to(
            self.context.project_path / "crackerjack",
        )
        test_name = f"test_{relative_path.stem}.py"

        return tests_dir / test_name

    async def _generate_test_content(
        self,
        module_file: Path,
        functions: list[dict[str, Any]],
        classes: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive test content with enhanced patterns."""
        module_name = self._get_module_import_path(module_file)
        module_category = self._categorize_module(
            str(module_file.relative_to(self.context.project_path))
        )

        # Generate enhanced header with appropriate imports
        base_content = self._generate_enhanced_test_file_header(
            module_name, module_file, module_category
        )

        # Generate function tests with parametrization
        function_tests = await self._generate_enhanced_function_tests(
            functions, module_category
        )

        # Generate class tests with fixtures
        class_tests = await self._generate_enhanced_class_tests(
            classes, module_category
        )

        # Add integration tests for certain module types
        integration_tests = await self._generate_integration_tests(
            module_file, functions, classes, module_category
        )

        return base_content + function_tests + class_tests + integration_tests

    def _generate_enhanced_test_file_header(
        self, module_name: str, module_file: Path, module_category: str
    ) -> str:
        """Generate enhanced test file header with appropriate imports based on module type."""
        # Determine imports based on module category
        imports = [
            "import pytest",
            "from pathlib import Path",
            "from unittest.mock import Mock, patch, AsyncMock",
        ]

        if module_category in ["service", "manager", "core"]:
            imports.append("import asyncio")

        if module_category == "agent":
            imports.extend(
                [
                    "from crackerjack.agents.base import AgentContext, FixResult, Issue, IssueType",
                ]
            )

        imports_str = "\n".join(imports)

        # Add specific imports for the module
        try:
            # Try to import specific classes/functions
            content = self.context.get_file_content(module_file) or ""
            tree = ast.parse(content)

            # Extract importable items
            importable_items = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef) and not node.name.startswith("_"):
                    importable_items.append(node.name)
                elif isinstance(
                    node, ast.FunctionDef | ast.AsyncFunctionDef
                ) and not node.name.startswith("_"):
                    importable_items.append(node.name)

            if importable_items:
                specific_imports = (
                    f"from {module_name} import {', '.join(importable_items[:10])}"
                )
            else:
                specific_imports = f"import {module_name}"

        except Exception:
            specific_imports = f"import {module_name}"

        class_name = f"Test{module_file.stem.replace('_', '').title()}"

        return f'''"""Tests for {module_name}.

This module contains comprehensive tests for {module_name} including:
- Basic functionality tests
- Edge case validation
- Error handling verification
- Integration testing
- Performance validation (where applicable)
"""

{imports_str}
{specific_imports}


class {class_name}:
    """Comprehensive test suite for {module_name}."""

    def test_module_imports_successfully(self):
        """Test that the module can be imported without errors."""
        import {module_name}
        assert {module_name} is not None

'''

    async def _generate_minimal_test_file(self, func_info: dict[str, Any]) -> str:
        file_path = Path(func_info["file"])
        module_name = self._get_module_import_path(file_path)

        return f'''"""Tests for {func_info["name"]} function."""

import pytest

from {module_name} import {func_info["name"]}


{await self._generate_function_test(func_info)}
'''

    def _get_module_import_path(self, file_path: Path) -> str:
        try:
            relative_path = file_path.relative_to(self.context.project_path)
            parts = (*relative_path.parts[:-1], relative_path.stem)
            return ".".join(parts)
        except ValueError:
            return file_path.stem

    async def _generate_function_test(self, func_info: dict[str, Any]) -> str:
        """Generate a test for a specific function."""
        func_name = func_info["name"]
        args = func_info.get("args", [])

        # Generate basic test template
        test_template = f'''def test_{func_name}_basic(self):
    """Test basic functionality of {func_name}."""
    try:
        # Basic test - may need manual implementation for specific arguments
        result = {func_name}({self._generate_default_args(args)})
        assert result is not None or result is None
    except TypeError:
        pytest.skip("Function requires specific arguments - manual implementation needed")
    except Exception as e:
        pytest.fail(f"Unexpected error in {func_name}: {{e}}")'''

        return test_template

    async def _generate_enhanced_function_tests(
        self, functions: list[dict[str, Any]], module_category: str
    ) -> str:
        """Generate enhanced test methods for functions with parametrization and edge cases."""
        if not functions:
            return ""

        test_methods = []
        for func in functions:
            func_name = func["name"]
            args = func.get("args", [])
            func.get("returns", "Any")

            # Generate basic test
            basic_test = await self._generate_basic_function_test(func, module_category)
            test_methods.append(basic_test)

            # Generate parametrized test if function has multiple args
            if len(args) > 1:
                parametrized_test = await self._generate_parametrized_test(
                    func, module_category
                )
                test_methods.append(parametrized_test)

            # Generate error handling test
            error_test = await self._generate_error_handling_test(func, module_category)
            test_methods.append(error_test)

            # Generate edge case tests for complex functions
            if len(args) > 2 or any(
                hint in func_name.lower()
                for hint in ["process", "validate", "parse", "convert"]
            ):
                edge_test = await self._generate_edge_case_test(func, module_category)
                test_methods.append(edge_test)

        return "\n".join(test_methods)

    async def _generate_basic_function_test(
        self, func: dict[str, Any], module_category: str
    ) -> str:
        """Generate basic functionality test for a function."""
        func_name = func["name"]
        args = func.get("args", [])

        # Generate test based on module category
        if module_category == "agent":
            test_template = f'''
    def test_{func_name}_basic_functionality(self):
        """Test basic functionality of {func_name}."""
        # TODO: Implement specific test logic for {func_name}
        # This is a placeholder test that should be customized
        try:
            result = {func_name}({self._generate_smart_default_args(args)})
            assert result is not None or result is None
        except (TypeError, NotImplementedError) as e:
            pytest.skip(f"Function {func_name} requires manual implementation: {{e}}")
        except Exception as e:
            pytest.fail(f"Unexpected error in {func_name}: {{e}}")'''

        elif module_category in ["service", "manager"]:
            test_template = f'''
    @pytest.mark.asyncio
    async def test_{func_name}_basic_functionality(self):
        """Test basic functionality of {func_name}."""
        # TODO: Implement specific test logic for {func_name}
        # Consider mocking external dependencies
        try:
            if asyncio.iscoroutinefunction({func_name}):
                result = await {func_name}({self._generate_smart_default_args(args)})
            else:
                result = {func_name}({self._generate_smart_default_args(args)})
            assert result is not None or result is None
        except (TypeError, NotImplementedError) as e:
            pytest.skip(f"Function {func_name} requires manual implementation: {{e}}")
        except Exception as e:
            pytest.fail(f"Unexpected error in {func_name}: {{e}}")'''

        else:
            test_template = f'''
    def test_{func_name}_basic_functionality(self):
        """Test basic functionality of {func_name}."""
        try:
            result = {func_name}({self._generate_smart_default_args(args)})
            assert result is not None or result is None
        except (TypeError, NotImplementedError) as e:
            pytest.skip(f"Function {func_name} requires manual implementation: {{e}}")
        except Exception as e:
            pytest.fail(f"Unexpected error in {func_name}: {{e}}")'''

        return test_template

    async def _generate_parametrized_test(
        self, func: dict[str, Any], module_category: str
    ) -> str:
        """Generate parametrized test for functions with multiple arguments."""
        func_name = func["name"]
        args = func.get("args", [])

        # Generate test parameters based on argument types
        test_cases = self._generate_test_parameters(args)

        if not test_cases:
            return ""

        parametrize_decorator = f"@pytest.mark.parametrize({test_cases})"

        test_template = f'''
    {parametrize_decorator}
    def test_{func_name}_with_parameters(self, {", ".join(args) if len(args) <= 5 else "test_input"}):
        """Test {func_name} with various parameter combinations."""
        try:
            if len({args}) <= 5:
                result = {func_name}({", ".join(args)})
            else:
                result = {func_name}(**test_input)
            # Basic assertion - customize based on expected behavior
            assert result is not None or result is None
        except (TypeError, ValueError) as expected_error:
            # Some parameter combinations may be invalid - this is expected
            pass
        except Exception as e:
            pytest.fail(f"Unexpected error with parameters: {{e}}")'''

        return test_template

    async def _generate_error_handling_test(
        self, func: dict[str, Any], module_category: str
    ) -> str:
        """Generate error handling test for a function."""
        func_name = func["name"]
        args = func.get("args", [])

        test_template = f'''
    def test_{func_name}_error_handling(self):
        """Test {func_name} error handling with invalid inputs."""
        # Test with None values
        with pytest.raises((TypeError, ValueError, AttributeError)):
            {func_name}({self._generate_invalid_args(args)})

        # Test with empty/invalid values where applicable
        if len({args}) > 0:
            with pytest.raises((TypeError, ValueError)):
                {func_name}({self._generate_edge_case_args(args, "empty")})'''

        return test_template

    async def _generate_edge_case_test(
        self, func: dict[str, Any], module_category: str
    ) -> str:
        """Generate edge case test for complex functions."""
        func_name = func["name"]
        args = func.get("args", [])

        test_template = f'''
    def test_{func_name}_edge_cases(self):
        """Test {func_name} with edge case scenarios."""
        # Test boundary conditions
        edge_cases = [
            {self._generate_edge_case_args(args, "boundary")},
            {self._generate_edge_case_args(args, "extreme")},
        ]

        for edge_case in edge_cases:
            try:
                result = {func_name}(*edge_case)
                # Verify the function handles edge cases gracefully
                assert result is not None or result is None
            except (ValueError, TypeError):
                # Some edge cases may be invalid - that's acceptable
                pass
            except Exception as e:
                pytest.fail(f"Unexpected error with edge case {{edge_case}}: {{e}}")'''

        return test_template

    def _generate_test_parameters(self, args: list[str]) -> str:
        """Generate test parameters for parametrized tests."""
        if not args or len(args) > 5:  # Limit complexity
            return ""

        # Simple parameter generation
        param_names = ", ".join(f'"{arg}"' for arg in args)
        param_values = []

        # Generate a few test cases
        for i in range(min(3, len(args))):
            test_case = []
            for arg in args:
                if "path" in arg.lower():
                    test_case.append(f'Path("test_{i}")')
                elif "str" in arg.lower() or "name" in arg.lower():
                    test_case.append(f'"test_{i}"')
                elif "int" in arg.lower() or "count" in arg.lower():
                    test_case.append(str(i))
                elif "bool" in arg.lower():
                    test_case.append("True" if i % 2 == 0 else "False")
                else:
                    test_case.append("None")
            param_values.append(f"({', '.join(test_case)})")

        return f"[{param_names}], [{', '.join(param_values)}]"

    def _generate_smart_default_args(self, args: list[str]) -> str:
        """Generate smarter default arguments based on argument names."""
        if not args or args == ["self"]:
            return ""

        filtered_args = [arg for arg in args if arg != "self"]
        if not filtered_args:
            return ""

        placeholders = []
        for arg in filtered_args:
            arg_lower = arg.lower()
            if any(term in arg_lower for term in ["path", "file"]):
                placeholders.append('Path("test_file.txt")')
            elif any(term in arg_lower for term in ["url", "uri"]):
                placeholders.append('"https://example.com"')
            elif any(term in arg_lower for term in ["email", "mail"]):
                placeholders.append('"test@example.com"')
            elif any(term in arg_lower for term in ["id", "uuid"]):
                placeholders.append('"test-id-123"')
            elif any(term in arg_lower for term in ["name", "title"]):
                placeholders.append('"test_name"')
            elif any(term in arg_lower for term in ["count", "size", "number", "num"]):
                placeholders.append("10")
            elif any(term in arg_lower for term in ["enable", "flag", "is_", "has_"]):
                placeholders.append("True")
            elif any(term in arg_lower for term in ["data", "content", "text"]):
                placeholders.append('"test data"')
            elif any(term in arg_lower for term in ["list", "items"]):
                placeholders.append('["test1", "test2"]')
            elif any(term in arg_lower for term in ["dict", "config", "options"]):
                placeholders.append('{"key": "value"}')
            else:
                placeholders.append('"test"')

        return ", ".join(placeholders)

    def _generate_invalid_args(self, args: list[str]) -> str:
        """Generate invalid arguments for error testing."""
        filtered_args = [arg for arg in args if arg != "self"]
        if not filtered_args:
            return ""
        return ", ".join(["None"] * len(filtered_args))

    def _generate_edge_case_args(self, args: list[str], case_type: str) -> str:
        """Generate edge case arguments."""
        filtered_args = [arg for arg in args if arg != "self"]
        if not filtered_args:
            return ""

        if case_type == "empty":
            placeholders = []
            for arg in filtered_args:
                if any(term in arg.lower() for term in ["str", "name", "text"]):
                    placeholders.append('""')
                elif any(term in arg.lower() for term in ["list", "items"]):
                    placeholders.append("[]")
                elif any(term in arg.lower() for term in ["dict", "config"]):
                    placeholders.append("{}")
                else:
                    placeholders.append("None")
        elif case_type == "boundary":
            placeholders = []
            for arg in filtered_args:
                if any(term in arg.lower() for term in ["count", "size", "number"]):
                    placeholders.append("0")
                elif any(term in arg.lower() for term in ["str", "name"]):
                    placeholders.append('"x" * 1000')  # Very long string
                else:
                    placeholders.append("None")
        else:  # extreme
            placeholders = []
            for arg in filtered_args:
                if any(term in arg.lower() for term in ["count", "size", "number"]):
                    placeholders.append("-1")
                else:
                    placeholders.append("None")

        return ", ".join(placeholders)

    async def _generate_enhanced_class_tests(
        self, classes: list[dict[str, Any]], module_category: str
    ) -> str:
        """Generate enhanced test methods for classes with fixtures and comprehensive coverage."""
        if not classes:
            return ""

        test_methods = []
        fixtures = []

        for cls in classes:
            cls["name"]
            methods = cls.get("methods", [])

            # Generate fixture for class instantiation
            fixture = await self._generate_class_fixture(cls, module_category)
            if fixture:
                fixtures.append(fixture)

            # Basic class instantiation test
            instantiation_test = await self._generate_class_instantiation_test(
                cls, module_category
            )
            test_methods.append(instantiation_test)

            # Generate tests for public methods
            for method in methods[:5]:  # Increased limit for better coverage
                method_test = await self._generate_class_method_test(
                    cls, method, module_category
                )
                test_methods.append(method_test)

            # Generate property tests if applicable
            property_test = await self._generate_class_property_test(
                cls, module_category
            )
            if property_test:
                test_methods.append(property_test)

        # Combine fixtures and tests
        fixture_section = "\n".join(fixtures) if fixtures else ""
        test_section = "\n".join(test_methods)

        return fixture_section + test_section

    async def _generate_class_fixture(
        self, cls: dict[str, Any], module_category: str
    ) -> str:
        """Generate pytest fixture for class instantiation."""
        class_name = cls["name"]

        if module_category in ["service", "manager", "core"]:
            # These often require dependency injection
            fixture_template = f'''
    @pytest.fixture
    def {class_name.lower()}_instance(self):
        """Fixture to create {class_name} instance for testing."""
        # TODO: Configure dependencies and mocks as needed
        try:
            return {class_name}()
        except TypeError:
            # If constructor requires arguments, mock them
            with patch.object({class_name}, '__init__', return_value=None):
                instance = {class_name}.__new__({class_name})
                return instance'''

        elif module_category == "agent":
            # Agents typically require AgentContext
            fixture_template = f'''
    @pytest.fixture
    def {class_name.lower()}_instance(self):
        """Fixture to create {class_name} instance for testing."""
        # Mock AgentContext for agent testing
        mock_context = Mock(spec=AgentContext)
        mock_context.project_path = Path("/test/project")
        mock_context.get_file_content = Mock(return_value="# test content")
        mock_context.write_file_content = Mock(return_value=True)

        try:
            return {class_name}(mock_context)
        except Exception:
            pytest.skip("Agent requires specific context configuration")'''

        else:
            # Simple fixture for other classes
            fixture_template = f'''
    @pytest.fixture
    def {class_name.lower()}_instance(self):
        """Fixture to create {class_name} instance for testing."""
        try:
            return {class_name}()
        except TypeError:
            pytest.skip("Class requires specific constructor arguments")'''

        return fixture_template

    async def _generate_class_instantiation_test(
        self, cls: dict[str, Any], module_category: str
    ) -> str:
        """Generate class instantiation test."""
        class_name = cls["name"]

        test_template = f'''
    def test_{class_name.lower()}_instantiation(self, {class_name.lower()}_instance):
        """Test successful instantiation of {class_name}."""
        assert {class_name.lower()}_instance is not None
        assert isinstance({class_name.lower()}_instance, {class_name})

        # Test basic attributes exist
        assert hasattr({class_name.lower()}_instance, '__class__')
        assert {class_name.lower()}_instance.__class__.__name__ == "{class_name}"'''

        return test_template

    async def _generate_class_method_test(
        self, cls: dict[str, Any], method_name: str, module_category: str
    ) -> str:
        """Generate test for a class method."""
        class_name = cls["name"]

        if module_category == "agent" and method_name in [
            "can_handle",
            "analyze_and_fix",
        ]:
            # Special handling for agent methods
            test_template = f'''
    @pytest.mark.asyncio
    async def test_{class_name.lower()}_{method_name}(self, {class_name.lower()}_instance):
        """Test {class_name}.{method_name} method."""
        if "{method_name}" == "can_handle":
            # Test with mock issue
            mock_issue = Mock(spec=Issue)
            mock_issue.type = IssueType.COVERAGE_IMPROVEMENT
            mock_issue.message = "test coverage issue"
            mock_issue.file_path = "/test/path.py"

            result = await {class_name.lower()}_instance.{method_name}(mock_issue)
            assert isinstance(result, (int, float))
            assert 0.0 <= result <= 1.0

        elif "{method_name}" == "analyze_and_fix":
            # Test with mock issue
            mock_issue = Mock(spec=Issue)
            mock_issue.type = IssueType.COVERAGE_IMPROVEMENT
            mock_issue.message = "test coverage issue"
            mock_issue.file_path = "/test/path.py"

            result = await {class_name.lower()}_instance.{method_name}(mock_issue)
            assert isinstance(result, FixResult)
            assert hasattr(result, 'success')
            assert hasattr(result, 'confidence')'''

        elif module_category in ["service", "manager"]:
            test_template = f'''
    @pytest.mark.asyncio
    async def test_{class_name.lower()}_{method_name}(self, {class_name.lower()}_instance):
        """Test {class_name}.{method_name} method."""
        try:
            method = getattr({class_name.lower()}_instance, "{method_name}", None)
            assert method is not None, f"Method {method_name} should exist"

            # Test method call (may need arguments)
            if asyncio.iscoroutinefunction(method):
                result = await method()
            else:
                result = method()

            # Basic assertion - customize based on expected behavior
            assert result is not None or result is None

        except (TypeError, NotImplementedError):
            pytest.skip(f"Method {method_name} requires specific arguments or implementation")
        except Exception as e:
            pytest.fail(f"Unexpected error in {method_name}: {{e}}")'''

        else:
            test_template = f'''
    def test_{class_name.lower()}_{method_name}(self, {class_name.lower()}_instance):
        """Test {class_name}.{method_name} method."""
        try:
            method = getattr({class_name.lower()}_instance, "{method_name}", None)
            assert method is not None, f"Method {method_name} should exist"

            # Test method call
            result = method()
            assert result is not None or result is None

        except (TypeError, NotImplementedError):
            pytest.skip(f"Method {method_name} requires specific arguments or implementation")
        except Exception as e:
            pytest.fail(f"Unexpected error in {method_name}: {{e}}")'''

        return test_template

    async def _generate_class_property_test(
        self, cls: dict[str, Any], module_category: str
    ) -> str:
        """Generate test for class properties."""
        class_name = cls["name"]

        # Only generate property tests for certain module categories
        if module_category not in ["service", "manager", "agent"]:
            return ""

        test_template = f'''
    def test_{class_name.lower()}_properties(self, {class_name.lower()}_instance):
        """Test {class_name} properties and attributes."""
        # Test that instance has expected structure
        assert hasattr({class_name.lower()}_instance, '__dict__') or hasattr({class_name.lower()}_instance, '__slots__')

        # Test string representation
        str_repr = str({class_name.lower()}_instance)
        assert len(str_repr) > 0
        assert "{class_name}" in str_repr or "{class_name.lower()}" in str_repr.lower()'''

        return test_template

    async def _generate_integration_tests(
        self,
        module_file: Path,
        functions: list[dict[str, Any]],
        classes: list[dict[str, Any]],
        module_category: str,
    ) -> str:
        """Generate integration tests for certain module types."""
        if module_category not in ["service", "manager", "core"]:
            return ""

        # Only generate integration tests for modules with sufficient complexity
        if len(functions) < 3 and len(classes) < 2:
            return ""

        integration_tests = f'''

    # Integration Tests
    @pytest.mark.integration
    def test_{module_file.stem}_integration(self):
        """Integration test for {module_file.stem} module functionality."""
        # TODO: Implement integration test scenarios
        # Test interactions between classes and functions in this module
        pytest.skip("Integration test needs manual implementation")

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_{module_file.stem}_async_integration(self):
        """Async integration test for {module_file.stem} module."""
        # TODO: Implement async integration scenarios
        # Test async workflows and dependencies
        pytest.skip("Async integration test needs manual implementation")

    @pytest.mark.performance
    def test_{module_file.stem}_performance(self):
        """Basic performance test for {module_file.stem} module."""
        # TODO: Add performance benchmarks if applicable
        # Consider timing critical operations
        pytest.skip("Performance test needs manual implementation")'''

        return integration_tests

    def _generate_default_args(self, args: list[str]) -> str:
        """Generate default arguments for function calls."""
        if not args or args == ["self"]:
            return ""

        # Filter out 'self' parameter
        filtered_args = [arg for arg in args if arg != "self"]
        if not filtered_args:
            return ""

        # Generate placeholder arguments
        placeholders = []
        for arg in filtered_args:
            if "path" in arg.lower():
                placeholders.append('Path("test")')
            elif "str" in arg.lower() or "name" in arg.lower():
                placeholders.append('"test"')
            elif "int" in arg.lower() or "count" in arg.lower():
                placeholders.append("1")
            elif "bool" in arg.lower():
                placeholders.append("True")
            else:
                placeholders.append("None")

        return ", ".join(placeholders)


agent_registry.register(TestCreationAgent)
